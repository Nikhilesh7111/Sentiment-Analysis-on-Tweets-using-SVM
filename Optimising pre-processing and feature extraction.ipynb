{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising pre-processing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage):\n",
    "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
    "    and performs the preprocessing.\"\"\"\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        train_data.append((to_feature_vector(pre_process(text)),label))\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        test_data.append((to_feature_vector(pre_process(text)),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_line(line):\n",
    "    if len(line) >= 2:\n",
    "        label= line[1]\n",
    "        statement =line[2]  \n",
    "    else:\n",
    "        \n",
    "        label, statement = None, None  \n",
    "    return label, statement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', ',', '@', 'hillaryclinton', '!', 'always', '#', 'stillwithher', '!', 'üíô', 'deserved', 'win', '!', '#', 'recount', '#', 'faithlesselectors', '#', 'auditthevote', 'hero', '!', 'üíô']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words_function = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pre_process(text):\n",
    "    # Separate punctuation and tokenize\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"‚Äù\\)])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"([.,;:!?'\\\"‚Äú\\(\\)])(\\w)\", r\"\\1 \\2\", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatization and stop word removal\n",
    "    tokens = [lemmatizer.lemmatize(t.lower()) for t in tokens if t.lower() not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "def extract_pos_tags(tokens):\n",
    "    return [pos for _, pos in nltk.pos_tag(tokens)]\n",
    "\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def extract_dependency_relations(text):\n",
    "#     doc = nlp(text)\n",
    "#     return [(token.text, token.dep_) for token in doc]\n",
    "\n",
    "# \n",
    "print(pre_process(\"We love you,@HillaryClinton! We are always #StillWithHer! üíô You deserved to win!#recount #faithlesselectors #AuditTheVote You are a hero! üíô\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1}\n"
     ]
    }
   ],
   "source": [
    "global_feature_dict = {}  # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens):\n",
    "    feature_vector = {}\n",
    "\n",
    "    def add_feature_n(feature):\n",
    "        if feature not in global_feature_dict:\n",
    "            global_feature_dict[feature] = len(global_feature_dict)\n",
    "        feature_index = global_feature_dict[feature]\n",
    "        feature_vector[feature_index] = feature_vector.get(feature_index, 0) + 1\n",
    "\n",
    "    # Adding unigrams\n",
    "    for word in tokens:\n",
    "        add_feature_n(word)\n",
    "\n",
    "    # Adding bigrams\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i + 1])\n",
    "        add_feature_n(bigram)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "# Example usage\n",
    "tokens = [\"this\", \"is\", \"a\", \"sample\", \"text\"]\n",
    "feature_vector = to_feature_vector(tokens)\n",
    "print(feature_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "def train_classifier(data):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_results(results):\n",
    "\n",
    "    sum_precision = 0\n",
    "    sum_recall = 0\n",
    "    sum_f1_score = 0\n",
    "    n = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        \n",
    "        sum_precision += result['weighted avg']['precision']\n",
    "        sum_recall += result['weighted avg']['recall']\n",
    "        sum_f1_score += result['weighted avg']['f1-score']\n",
    "\n",
    "    \n",
    "    avg_precision = sum_precision / n\n",
    "    avg_recall = sum_recall / n\n",
    "    avg_f1_score = sum_f1_score / n\n",
    "    print('Average Precision:', avg_precision)\n",
    "    print('Average Recall:', avg_recall)\n",
    "    print('Average F1 Score:', avg_f1_score)\n",
    "    return avg_precision, avg_recall, avg_f1_score\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def cross_validate(dataset, folds):\n",
    "    results = []\n",
    "    fold_size = int(len(dataset) / folds) + 1\n",
    "\n",
    "    for i in range(0, len(dataset), fold_size):\n",
    "        test_data = dataset[i:i + fold_size]\n",
    "        train_data = dataset[:i] + dataset[i + fold_size:]\n",
    "\n",
    "        classifier = train_classifier(train_data)\n",
    "        predicted = predict_labels([sample[0] for sample in test_data], classifier)\n",
    "        actual = [sample[1] for sample in test_data]\n",
    "\n",
    "        results.append(classification_report(actual, predicted, output_dict=True))\n",
    "        print(\"Fold start on items %d - %d\" % (i, i + fold_size))\n",
    "\n",
    "    return cv_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    return classifier.classify_many(samples)\n",
    "\n",
    "def predict_label_from_raw(sample, classifier):\n",
    "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "    processed_sample = to_feature_vector(pre_process(sample))\n",
    "    return classifier.classify(to_feature_vector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 33539 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 33539 rawData, 26831 trainData, 6708 testData\n",
      "Training Samples: \n",
      "26831\n",
      "Features: \n",
      "350389\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loading reviews\n",
    "def load_data(path, raw_data):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "      reader = csv.reader(f, delimiter='\\t')\n",
    "      next(reader, None)  # Skip header\n",
    "      for line in reader:\n",
    "       label, text = parse_data_line(line)\n",
    "       raw_data.append((text, label))\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "def split_and_preprocess_data(raw_data, percentage):\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int(percentage * num_samples)\n",
    "\n",
    "    # Make sure to create a tuple (featureset, label) for each sample\n",
    "    train_data = [(to_feature_vector(pre_process(text)), label) for text, label in raw_data[:num_training_samples]]\n",
    "    test_data = [(to_feature_vector(pre_process(text)), label) for text, label in raw_data[num_training_samples:]]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "\n",
    "# references to the data files\n",
    "data_file_path = 'sentiment-dataset.tsv'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "load_data(data_file_path,raw_data) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "train_data, test_data=split_and_preprocess_data(raw_data,0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "pred=cross_validate(train_data, 10)  #¬†will work and output overall performance of p, r, f-score when cv implemented\n",
    "\n",
    "print(pred)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({148: 1, 44: 2, 2421: 1, 142016: 1, 140: 1, 118: 1, 292323: 1, 292324: 1, 9: 1, 292325: 1, 146: 2, 287: 1, 85401: 1, 7661: 1, 5593: 1, 51855: 1, 292326: 1, 292327: 1, 50902: 1, 292328: 1, 292329: 1, 292330: 1, 292331: 1, 292332: 1, 793: 1, 17237: 1, 292333: 1, 292334: 1, 205215: 1}, 'positive')\n",
      "Training Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training!\n",
      "Precision: 0.859621\n",
      "Recall: 0.861061\n",
      "F Score:0.859627\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the traning data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(test_data[0])   # have a look at the first test data instance\n",
    "    classifier = train_classifier(train_data)  # train the classifier\n",
    "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
    "    test_pred = predict_labels([x[0] for x in test_data], classifier)  #¬†classify the test data to get predicted labels\n",
    "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
